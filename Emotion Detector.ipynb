{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as functional\n",
    "import torch.optim as optim\n",
    "from PIL import Image\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNetwork, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=9, kernel_size=5)\n",
    "        self.conv1_bn = nn.BatchNorm2d(9)\n",
    "        self.conv2 = nn.Conv2d(in_channels=9, out_channels=18, kernel_size=5)\n",
    "        self.conv2_bn = nn.BatchNorm2d(18)\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features=18*9*9, out_features=128)\n",
    "        self.fc2 = nn.Linear(in_features=128, out_features=64)\n",
    "        self.out = nn.Linear(in_features=64, out_features=7)\n",
    "        \n",
    "    def forward(self, t):\n",
    "        t = self.conv1(t)\n",
    "        t = self.conv1_bn(t)\n",
    "        t = functional.relu(t)\n",
    "        t = functional.max_pool2d(t, kernel_size=2, stride=2)\n",
    "        \n",
    "        t = self.conv2(t)\n",
    "        t = self.conv2_bn(t)\n",
    "        t = functional.relu(t)\n",
    "        t = functional.max_pool2d(t, kernel_size=2,  stride=2)\n",
    "\n",
    "        t = functional.relu(self.fc1(t.reshape(-1, 18*9*9)))\n",
    "        t = functional.relu(self.fc2(t))\n",
    "        t = self.out(t)\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_photo():\n",
    "    cam = cv2.VideoCapture(0)\n",
    "    s, img = cam.read()\n",
    "    if s:\n",
    "        cv2.namedWindow(\"cam-test\", cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow(\"cam-test\", img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    crop = img[200:400, 250:450]\n",
    "    cv2.imwrite(\"cam-photo.jpg\", crop)\n",
    "\n",
    "\n",
    "def image_to_tensor(fname):\n",
    "    pil_img = Image.open(fname).convert('L') # convert to greyscale\n",
    "    pil_img.thumbnail((48,48))\n",
    "    t = transforms.ToTensor()(pil_img)\n",
    "    t = torch.stack([t for i in range(32)])\n",
    "    return t\n",
    "\n",
    "def guess_emotion(t):\n",
    "    network = ConvolutionalNetwork()\n",
    "    network.load_state_dict(torch.load(\"C:/Users/Nikhil Deenamsetty/MachineLearning/NNModels/EmotionCNNV1.pth\", map_location='cpu'))\n",
    "    labels = [\"Angry\", \"Disgust\", \"Fear\", \"Happy\", \"Sad\", \"Surprise\", \"Neutral\"]\n",
    "    with torch.no_grad():\n",
    "        out = network(t)\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "    \n",
    "    return labels[predicted[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Happy'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "take_photo()\n",
    "t = image_to_tensor(\"cam-photo.jpg\")\n",
    "guess_emotion(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
